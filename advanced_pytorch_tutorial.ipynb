{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Tutorial on Advanced PyTorch Techniques\n",
    "\n",
    "**Companion Notebook for the Advanced PyTorch Presentation in this Repo**\n",
    "\n",
    "This notebook provides a hands-on walkthrough of the concepts discussed in the presentation. Here, you'll find executable code for custom modules, loss functions, advanced training loops, and more. The goal is to bridge the gap between theory and implementation, enabling you to apply these techniques directly into your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Foundations Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and Autograd\n",
    "\n",
    "Everything in PyTorch is built upon the `Tensor`. Let's start with a quick refresher on creating tensors and tracking gradients with `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Gradient for w: tensor([[-1.1562,  1.3740,  0.4931]])\n",
      "Gradient for b: tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Create tensors, specifying that we want to track their gradients\n",
    "x = torch.randn(3, 1, requires_grad=True, device=device)\n",
    "w = torch.randn(1, 3, requires_grad=True, device=device)\n",
    "b = torch.randn(1, 1, requires_grad=True, device=device)\n",
    "\n",
    "# Build a simple computation graph (y = w * x + b)\n",
    "y = torch.matmul(w, x) + b\n",
    "\n",
    "# Define a dummy loss\n",
    "loss = torch.sum(y)\n",
    "\n",
    "# Backpropagate to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# The gradients are now stored in the .grad attribute of each tensor\n",
    "print(\"Gradient for w:\", w.grad)\n",
    "print(\"Gradient for b:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with CUDA\n",
    "\n",
    "While `device = 'cuda'` works, it's useful to understand the specifics of GPU operations. The most common source of error is a \"device mismatch,\" where tensors involved in an operation live on different devices (e.g., one on CPU, one on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    # Tensors must be explicitly moved to the GPU\n",
    "    cpu_tensor = torch.randn(3, 3)\n",
    "    print(f\"CPU Tensor Device: {cpu_tensor.device}\")\n",
    "    \n",
    "    gpu_tensor = cpu_tensor.to(device)\n",
    "    print(f\"GPU Tensor Device: {gpu_tensor.device}\")\n",
    "    \n",
    "    # This will cause a runtime error:\n",
    "    try:\n",
    "        result = cpu_tensor + gpu_tensor\n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "    \n",
    "    # The fix is to ensure all tensors are on the same device\n",
    "    gpu_tensor_2 = torch.randn(3, 3).to(device)\n",
    "    result = gpu_tensor + gpu_tensor_2\n",
    "    print(f\"\\nSuccessful operation. Result device: {result.device}\")\n",
    "    \n",
    "    # Asynchronous Execution: CUDA operations are asynchronous.\n",
    "    # This means the Python code continues executing without waiting for the GPU operation to finish.\n",
    "    # To accurately benchmark code, you need to synchronize.\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    large_tensor_a = torch.randn(10000, 10000, device=device)\n",
    "    large_tensor_b = torch.randn(10000, 10000, device=device)\n",
    "    c = large_tensor_a @ large_tensor_b\n",
    "    torch.cuda.synchronize() # Wait for the GPU operation to complete\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\nTime with synchronization: {end_time - start_time:.4f} seconds\")\n",
    "else:\n",
    "    print(\"CUDA not available. Skipping CUDA-specific section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `nn.Module`\n",
    "\n",
    "The `nn.Module` is the cornerstone of building models. It automatically tracks parameters and provides helpful utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleModel(\n",
      "  (layer1): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (layer2): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n",
      "layer1.weight torch.Size([20, 10])\n",
      "layer1.bias torch.Size([20])\n",
      "layer2.weight torch.Size([5, 20])\n",
      "layer2.bias torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        # Define layers. Their parameters are automatically registered.\n",
    "        self.layer1 = nn.Linear(in_features=10, out_features=20)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(in_features=20, out_features=5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the data flow\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and move it to the selected device\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# PyTorch provides a clean summary of the model architecture\n",
    "print(model)\n",
    "\n",
    "# You can easily inspect all model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Custom Architectures"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Custom ResNet Block\n",
    "\n",
    "Research often requires implementing architectures that don't fit into a simple sequential pattern. A residual block, with its skip connection, is a perfect example. We define the layers in `__init__` and the complex data flow (including the addition of the skip connection) in `forward`.\n",
    "\n",
    "As a refresher, the basic block in ResNet architecture looks like this:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "_Snippet of a ResNet50 model's skip connection architecture visualized using Netron_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 32, 32])\n",
      "Output shape: torch.Size([2, 128, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # Main path\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut path to handle dimension changes (if stride is not 1 or channels change)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x) # The skip connection is just an addition\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Test the block with a random input tensor\n",
    "input_tensor = torch.randn(2, 64, 32, 32).to(device) # (batch, channels, height, width)\n",
    "resnet_block = BasicBlock(in_channels=64, out_channels=128, stride=2).to(device)\n",
    "output_tensor = resnet_block(input_tensor)\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "print(\"Output shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Multi-Branch (Inception-like) Module\n",
    "\n",
    "This module processes an input through several parallel convolutional paths and then concatenates the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 32, 32])\n",
      "Output shape: torch.Size([2, 96, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels_1x1, out_channels_3x3, out_channels_5x5):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.branch1 = nn.Conv2d(in_channels, out_channels_1x1, kernel_size=1)\n",
    "        self.branch2 = nn.Conv2d(in_channels, out_channels_3x3, kernel_size=3, padding=1)\n",
    "        self.branch3 = nn.Conv2d(in_channels, out_channels_5x5, kernel_size=5, padding=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Process input through each branch\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        # Concatenate results along the channel dimension (dim=1)\n",
    "        return torch.cat([out1, out2, out3], dim=1)\n",
    "\n",
    "input_tensor = torch.randn(2, 64, 32, 32).to(device)\n",
    "inception_block = InceptionModule(64, 32, 48, 16).to(device)\n",
    "output_tensor = inception_block(input_tensor)\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "# The output channels should be the sum of the branch output channels (32 + 48 + 16 = 96)\n",
    "print(\"Output shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Custom Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Triplet Loss\n",
    "\n",
    "Most custom losses can be implemented as simple Python functions that use differentiable PyTorch operations. Here is the triplet loss for metric learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss: 1.013861060142517\n"
     ]
    }
   ],
   "source": [
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    \"\"\"Calculates the triplet loss.\"\"\"\n",
    "    distance_positive = F.pairwise_distance(anchor, positive, p=2)\n",
    "    distance_negative = F.pairwise_distance(anchor, negative, p=2)\n",
    "    \n",
    "    # We want d(a, p) + margin < d(a, n)  =>  d(a, p) - d(a, n) + margin < 0\n",
    "    losses = F.relu(distance_positive - distance_negative + margin)\n",
    "    \n",
    "    return losses.mean()\n",
    "\n",
    "# Create dummy embeddings\n",
    "anchor_embedding = torch.randn(10, 128, device=device)\n",
    "positive_embedding = torch.randn(10, 128, device=device)\n",
    "negative_embedding = torch.randn(10, 128, device=device)\n",
    "\n",
    "# Calculate loss\n",
    "loss = triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n",
    "print(f\"Triplet Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Focal Loss\n",
    "\n",
    "Focal loss is another great example of a custom loss, designed to handle class imbalance by focusing on hard-to-classify examples. It's a modification of standard cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal Loss: 0.3142664432525635\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs are model logits, targets are class indices\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt)**self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Dummy data\n",
    "logits = torch.randn(10, 5, device=device) # 10 samples, 5 classes\n",
    "labels = torch.randint(0, 5, (10,), device=device)\n",
    "\n",
    "focal_loss_fn = FocalLoss()\n",
    "loss = focal_loss_fn(logits, labels)\n",
    "print(f\"Focal Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Advanced Training Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and Schedulers\n",
    "\n",
    "Choosing the right optimizer and learning rate schedule is critical. `AdamW` is the modern default for many tasks. `CosineAnnealingLR` is a powerful and popular scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Learning Rate: 0.001\n",
      "Learning Rate after 10 steps: 0.0009755282581475767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jainarchita/miniconda3/envs/ai_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# The recommended optimizer for Transformers and other modern architectures\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "# A scheduler that smoothly varies the learning rate\n",
    "# T_max is the number of iterations for one half of a cosine cycle\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100) \n",
    "\n",
    "print(\"Initial Learning Rate:\", scheduler.get_last_lr()[0])\n",
    "\n",
    "# Simulate 10 training steps\n",
    "for i in range(10):\n",
    "    # In a real loop, you'd do: optimizer.step(); scheduler.step()\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Learning Rate after 10 steps:\", scheduler.get_last_lr()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Accumulation\n",
    "\n",
    "This technique lets you simulate a larger batch size than your GPU memory can handle. It's essential for training large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Optimizer step (effective batch size: 32)\n",
      "Step 8: Optimizer step (effective batch size: 32)\n",
      "Step 12: Optimizer step (effective batch size: 32)\n",
      "Step 16: Optimizer step (effective batch size: 32)\n"
     ]
    }
   ],
   "source": [
    "accumulation_steps = 4\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "dummy_dataloader = [(torch.randn(8, 10), torch.randn(8,5)) for _ in range(16)] # Batch size of 8\n",
    "\n",
    "optimizer.zero_grad() # Reset gradients once before the loop\n",
    "\n",
    "for i, (inputs, targets) in enumerate(dummy_dataloader):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = nn.MSELoss()(outputs, targets)\n",
    "    \n",
    "    # Scale loss for accumulation\n",
    "    loss = loss / accumulation_steps\n",
    "    \n",
    "    # Backward pass (gradients are accumulated)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights only after accumulating gradients for `accumulation_steps` batches\n",
    "    if (i + 1) % accumulation_steps == 0:\n",
    "        print(f\"Step {i+1}: Optimizer step (effective batch size: {8 * accumulation_steps})\")\n",
    "        optimizer.step()       # Update weights\n",
    "        optimizer.zero_grad()  # Reset gradients for the next accumulation cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Precision Training with `torch.cuda.amp`\n",
    "\n",
    "Automatic Mixed Precision (AMP) significantly speeds up training and reduces memory usage on modern NVIDIA GPUs by using `float16` for certain operations. The implementation is remarkably simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMP requires a CUDA-enabled GPU.\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Only runs on CUDA-enabled devices\n",
    "if device == 'cuda':\n",
    "    model = SimpleModel().to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # Create a gradient scaler\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Dummy data\n",
    "    inputs = torch.randn(8, 10, device=device)\n",
    "    targets = torch.randn(8, 5, device=device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Use the autocast context manager\n",
    "    with autocast():\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "    \n",
    "    # scaler.scale(loss) scales the loss. Gradients are unscaled by scaler.step().\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    print(\"Mixed precision training step completed.\")\n",
    "else:\n",
    "    print(\"AMP requires a CUDA-enabled GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Training with `DistributedDataParallel` (DDP)\n",
    "\n",
    "`DistributedDataParallel` (DDP) is the standard, most performant way to do multi-GPU training. Unlike `DataParallel`, it spawns a separate process for each GPU, eliminating the bottlenecks of the older method.\n",
    "\n",
    "**Important:** DDP scripts cannot be run directly inside a notebook because they require multiple processes to be launched from a script. Below is a complete, self-contained Python script. To run it:\n",
    "\n",
    "1.  Save the code below as a Python file (e.g., `ddp_example.py`).\n",
    "2.  From your terminal, run it using `torchrun`. If you have 2 GPUs, you would run:\n",
    "    ```bash\n",
    "    torchrun --nproc_per_node=2 ddp_example.py\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_example.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    \"\"\"Initializes the distributed process group.\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    # Initializes the distributed backend (nccl is recommended for NVIDIA GPUs)\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Cleans up the distributed process group.\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "class TrivialDataset(Dataset):\n",
    "    \"\"\"A simple dataset for demonstration purposes.\"\"\"\n",
    "    def __init__(self, size=128):\n",
    "        self.size = size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.randn(10), torch.randn(5)\n",
    "\n",
    "def main():\n",
    "    # DDP environment variables are set by torchrun\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    \n",
    "    setup(local_rank, world_size)\n",
    "    \n",
    "    print(f\"Running DDP on rank {local_rank}.\")\n",
    "\n",
    "    # 1. Create model and move it to the correct device for this process\n",
    "    model = nn.Linear(10, 5).to(local_rank)\n",
    "    # 2. Wrap the model with DDP\n",
    "    ddp_model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    # 3. Use DistributedSampler to ensure each process gets a different slice of data\n",
    "    dataset = TrivialDataset()\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=local_rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, sampler=sampler)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(2):\n",
    "        # The sampler needs to be informed of the current epoch\n",
    "        sampler.set_epoch(epoch)\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.to(local_rank), labels.to(local_rank)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(data)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward() # Gradients are automatically averaged across all processes\n",
    "            optimizer.step()\n",
    "        print(f\"Rank {local_rank}, Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Advanced PyTorch Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hooks\n",
    "\n",
    "Hooks are powerful tools for debugging and analysis, allowing you to inspect activations or gradients without modifying your model's code. Here, we'll use a forward hook to get the output of an intermediate layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_features = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    # This is the hook function\n",
    "    def hook(model, input, output):\n",
    "        activation_features[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model = SimpleModel().to(device)\n",
    "\n",
    "# Register a forward hook on the first linear layer's output (after the activation)\n",
    "model.activation.register_forward_hook(get_activation('relu1'))\n",
    "\n",
    "# Run a forward pass\n",
    "output = model(torch.randn(1, 10, device=device))\n",
    "\n",
    "# The hook has now captured the output of the ReLU layer\n",
    "print(\"Shape of captured ReLU activation:\", activation_features['relu1'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.compile` (PyTorch 2.x)\n",
    "\n",
    "The simplest way to get a significant performance boost. `torch.compile` JIT-compiles your model into an optimized graph, often with no code changes required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel().to(device)\n",
    "\n",
    "# This single line can provide significant speedups\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "# Now, just use the compiled model as you would the original\n",
    "input_tensor = torch.randn(16, 10, device=device)\n",
    "output = compiled_model(input_tensor)\n",
    "\n",
    "print(\"Forward pass with compiled model completed.\")\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Specialized Architectures (Further Reading)\n",
    "\n",
    "While implementing these from scratch is beyond the scope of this tutorial, PyTorch has a rich ecosystem of libraries that provide optimized building blocks for specialized domains.\n",
    "\n",
    "- **Graph Neural Networks:** For working with graph-structured data, the go-to library is [**PyTorch Geometric**](https://pytorch-geometric.readthedocs.io/en/latest/).\n",
    "\n",
    "- **Transformers:** For NLP and Vision Transformers, the [**Hugging Face `transformers`**](https://huggingface.co/docs/transformers/index) library is the undisputed industry standard, providing thousands of pre-trained models and easy-to-use APIs.\n",
    "\n",
    "- **Reinforcement Learning & Meta-Learning:** PyTorch's dynamic graph is ideal for these domains. While there are many libraries, understanding the core principles from this notebook will allow you to build custom training loops required for RL and MAML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has covered the practical implementation of several advanced PyTorch techniques. By mastering `nn.Module` subclassing, custom loss functions, and modern training strategies, you can build custom NN architecture for your Deep Learning Experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
